{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Il7H73VGJ2No"
   },
   "source": [
    "# Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3030,
     "status": "ok",
     "timestamp": 1532739744219,
     "user": {
      "displayName": "Hugo Neves",
      "photoUrl": "//lh6.googleusercontent.com/-yVVtZxPjhUQ/AAAAAAAAAAI/AAAAAAAAAfU/s1p3kfVzHaM/s50-c-k-no/photo.jpg",
      "userId": "115244738658251423663"
     },
     "user_tz": 180
    },
    "id": "DjbxoKABJ6K1",
    "outputId": "d043e801-c8f5-4b70-c5c8-a5c5753076f1"
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 16848,
     "status": "ok",
     "timestamp": 1532739761087,
     "user": {
      "displayName": "Hugo Neves",
      "photoUrl": "//lh6.googleusercontent.com/-yVVtZxPjhUQ/AAAAAAAAAAI/AAAAAAAAAfU/s1p3kfVzHaM/s50-c-k-no/photo.jpg",
      "userId": "115244738658251423663"
     },
     "user_tz": 180
    },
    "id": "abQOyifrLFBE",
    "outputId": "0c44d3cd-0f55-465c-dc4d-784066fdc9ac"
   },
   "outputs": [],
   "source": [
    "!rm -r ./mnist\n",
    "\n",
    "!wget https://www.dropbox.com/s/5yre1ofqco5titj/mnist.zip?dl=0 -O mnist.zip\n",
    "!unzip -q mnist.zip\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yNikfN0J2Np"
   },
   "source": [
    "## Setting data loader\n",
    "\n",
    "Creating data loader for reading images from the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2441,
     "status": "ok",
     "timestamp": 1532739763548,
     "user": {
      "displayName": "Hugo Neves",
      "photoUrl": "//lh6.googleusercontent.com/-yVVtZxPjhUQ/AAAAAAAAAAI/AAAAAAAAAfU/s1p3kfVzHaM/s50-c-k-no/photo.jpg",
      "userId": "115244738658251423663"
     },
     "user_tz": 180
    },
    "id": "YKPo231vJ2Nq",
    "outputId": "094a6226-54c0-44a3-acd5-6e9faa883dac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "from skimage import io\n",
    "\n",
    "# Constants.\n",
    "num_classes = 10\n",
    "root = './mnist/'\n",
    "\n",
    "# Class that reads a sequence of image paths from a text file and creates a data.Dataset with them.\n",
    "class CustomDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, fold, normalization = 'default'):\n",
    "\n",
    "        super(CustomDataset, self).__init__()\n",
    "        \n",
    "        # Initializing variables.\n",
    "        self.fold = fold\n",
    "        self.normalization = normalization\n",
    "\n",
    "        # Creating list of paths.\n",
    "        self.imgs = self.make_dataset()\n",
    "\n",
    "        # Check for consistency in list.\n",
    "        if len(self.imgs) == 0:\n",
    "\n",
    "            raise (RuntimeError('Found 0 images, please check the data set'))\n",
    "\n",
    "    def make_dataset(self):\n",
    "\n",
    "        # Initiating item list.\n",
    "        items = []\n",
    "\n",
    "        # Joining input paths.\n",
    "        img_path = os.path.join(root, self.fold)\n",
    "\n",
    "        # Reading paths from text file.\n",
    "        #data_list = [l.strip('\\n') for l in open(os.path.join(root, self.dataset, self.task + '_' + mode_str + '_f' + self.fold + '.txt')).readlines()]\n",
    "\n",
    "        # Reading paths from directory.\n",
    "        data_list = [f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))]\n",
    "        \n",
    "        # Creating list containing image and ground truth paths.\n",
    "        for it in data_list:\n",
    "            item = os.path.join(img_path, it)\n",
    "            items.append(item)\n",
    "\n",
    "        # Returning list.\n",
    "        return items\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Reading items from list.\n",
    "        img_path = self.imgs[index]\n",
    "        \n",
    "        # Reading images.\n",
    "        img = io.imread(img_path)\n",
    "        \n",
    "        # Reading label from image file.\n",
    "        lab = int(img_path[-5])\n",
    "        \n",
    "        # Removing unwanted channels. For the case of RGB images.\n",
    "        if len(img.shape) > 2:\n",
    "            img = img[:, :, 0] # Leaving only red (index = 0) channel from RGB.\n",
    "        \n",
    "        # Casting images to the appropriate dtypes.\n",
    "        img = img.astype(np.float32)\n",
    "        \n",
    "        # Normalization.\n",
    "        mn = img.min()\n",
    "        mx = img.max()\n",
    "        img = ((img - mn) / (mx - mn))\n",
    "        \n",
    "        # Adding channel dimension.\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        \n",
    "        # Turning to tensors.\n",
    "        img = torch.from_numpy(img)\n",
    "        \n",
    "        # Returning image and label to iterator.\n",
    "        return img, lab\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.imgs)\n",
    "\n",
    "# Setting data loader.\n",
    "batch_size = 100\n",
    "num_workers = 2 # Number of Threads on each data_loader.\n",
    "\n",
    "train_set = CustomDataset('train')\n",
    "train_loader = DataLoader(train_set, batch_size, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "test_set = CustomDataset('test')\n",
    "test_loader = DataLoader(test_set, batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "print('Size of training set: ' + str(len(train_set)) + ' samples')\n",
    "print('Size of test set: ' + str(len(test_set)) + ' samples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGtnadiqJ2Nu"
   },
   "source": [
    "## Setting architecture\n",
    "\n",
    "Creating a common CNN architecture composed of Convolutional and Fully Connected Layers.\n",
    "\n",
    "Links úteis:\n",
    "\n",
    "Função view() que dá reshape no tensor para adequá-lo ao tamanho da entrada das camadas. https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "\n",
    "Pacote torch.nn que contém as implementações das camadas. https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "P4oFGFAKJ2Nv"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Customized Network.\n",
    "class CustomNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_classes=10):\n",
    "\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        \n",
    "        # TO DO: Implementar a arquitetura de uma MLP.\n",
    "        # Exemplo de camada: self.layer1 = nn.Linear(args)\n",
    "        \n",
    "        \n",
    "        self.initialize_weights()\n",
    "    \n",
    "    # Function for randomly initializing weights.\n",
    "    def initialize_weights(self):\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TO DO: Implementar o forward.\n",
    "        # Deve retornar a saída da última camada da rede.\n",
    "        # A variável 'x' de entrada tem dimensões (BATCH_SIZE, N_CHANNELS, WIDTH, HEIGHT) = (100, 1, 28, 28).\n",
    "        # A entrada de uma camada fully connected deve ter dimensões (BATCH_SIZE, N_INPUTS) = (100, 1*28*28).\n",
    "        # A saída deve ter dimensões (BATCH_SIZE, N_CLASSES) = (100, 10).\n",
    "        # Exemplo de forward em uma camada: out1 = self.layer1(x)\n",
    "        # Um bom exercício é verificar o tamanho de todas as saídas da rede neural. Para verificar o tamanho de um tensor 'a', usar a função size: print(a.size()).\n",
    "        \n",
    "# Instancing Network.\n",
    "in_channels = 1 # The input images only contain 1 channel.\n",
    "num_classes = 10 # MNIST has 10 classes.\n",
    "\n",
    "# model = CustomNetwork(in_channels, num_classes) # CPU version.\n",
    "model = CustomNetwork(in_channels, num_classes).cuda() # GPU casting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOVXXfCiJ2Ny"
   },
   "source": [
    "## Setting optimizer\n",
    "\n",
    "$Pytorch$ has several options for optimizers, since the traditional SGD to more complex per-parameter adaptive ones (i.e. Adam, Adagrad, RSMProp...). All of them are located in the package torch.optim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "fhyopppRJ2Nz"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0001 # Learning rate.\n",
    "l2_normalization = 0.001 # L2 Normalization vai weight decay.\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999), weight_decay=l2_normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlBTGdzPJ2N1"
   },
   "source": [
    "## Setting loss criterion\n",
    "\n",
    "$CrossEntropyLoss$, as this is a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "H1WirBm_J2N2"
   },
   "outputs": [],
   "source": [
    "# Setting a classification loss.\n",
    "# criterion = nn.CrossEntropyLoss() # CPU version.\n",
    "criterion = nn.CrossEntropyLoss().cuda() # GPU casting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9uC1yHfJ2N4"
   },
   "source": [
    "## Training/Testing\n",
    "\n",
    "Iterating over epochs and batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2634
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 406603,
     "status": "ok",
     "timestamp": 1532740177960,
     "user": {
      "displayName": "Hugo Neves",
      "photoUrl": "//lh6.googleusercontent.com/-yVVtZxPjhUQ/AAAAAAAAAAI/AAAAAAAAAfU/s1p3kfVzHaM/s50-c-k-no/photo.jpg",
      "userId": "115244738658251423663"
     },
     "user_tz": 180
    },
    "id": "04-4qarbJ2N5",
    "outputId": "56f6b59f-2858-4931-c287-6e176bf5419b"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = 20 # Run network for 20 epochs.\n",
    "\n",
    "training_metrics = list() # List for accuracies in training procedure.\n",
    "test_metrics = list() # List for accuracies in test procedure.\n",
    "\n",
    "# Iterating over epochs.\n",
    "for ep in range(epochs):\n",
    "    \n",
    "    print('##############################################')\n",
    "    print('Starting epoch ' + str(ep + 1) + '/' + str(epochs) + '...')\n",
    "    \n",
    "    #####################################################################\n",
    "    # Training Procedure. ###############################################\n",
    "    #####################################################################\n",
    "    \n",
    "    print('    Training...')\n",
    "    \n",
    "    # Setting model to training mode.\n",
    "    model.train()\n",
    "    \n",
    "    total_correct = 0\n",
    "    \n",
    "    # Iterating over training batches.\n",
    "    for it, data in enumerate(train_loader):\n",
    "\n",
    "        # Obtaining images and labels for batch.\n",
    "        inps, labs = data\n",
    "        \n",
    "        # GPU casting. In CPU version, remove the following two lines.\n",
    "        inps = Variable(inps.cuda(), requires_grad=True)\n",
    "        labs = Variable(labs.cuda(), requires_grad=False) # requires_grad = False -> it isn't necessary to compute gradients from targets, only from losses.\n",
    "        \n",
    "        # Zeroing optimizer.\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # Forwarding inputs through DNN.\n",
    "        output = model(inps)\n",
    "        \n",
    "        # Computing loss according to network prediction for batch and targets.\n",
    "        # The Cross Entropy loss receives an output of size (BATCH_SIZE, N_CLASSES) and a label vector of size (BATCH_SIZE).\n",
    "        loss = criterion(output, labs)\n",
    "        \n",
    "        # Backpropagating loss.\n",
    "        loss.backward() # All backward pass is computed from this line automatically by package torch.autograd.\n",
    "        \n",
    "        # Taking optimization step.\n",
    "        opt.step()\n",
    "        \n",
    "        # Computing prediction to batch\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # Computing accuracy for batch.\n",
    "        correct = pred.eq(labs.view_as(pred)).sum().item()\n",
    "        accuracy = (100.0 * float(correct) / inps.size(0))\n",
    "        \n",
    "        training_metrics.append(accuracy)\n",
    "        \n",
    "        # Updating total counter.\n",
    "        total_correct += correct\n",
    "        \n",
    "    # Computing accuracy for whole epoch.\n",
    "    accuracy = 100.0 * float(total_correct) / len(train_set)\n",
    "    \n",
    "    print('        Accuracy for training epoch [' + str(ep + 1) + ' / ' + str(epochs) + ']: ' + str(accuracy))\n",
    "    \n",
    "    #####################################################################\n",
    "    # Testing Procedure.  ###############################################\n",
    "    #####################################################################\n",
    "    \n",
    "    print('    Testing...')\n",
    "        \n",
    "    # Setting model to evaluation mode.\n",
    "    model.eval()\n",
    "    \n",
    "    # GPU casting. In CPU version, remove the following two lines.\n",
    "    #inps = Variable(inps.cuda(), requires_grad=True)\n",
    "    #labs = Variable(labs.cuda(), requires_grad=True)\n",
    "    \n",
    "    total_correct = 0\n",
    "\n",
    "    # Iterating over test batches.\n",
    "    for it, data in enumerate(test_loader):\n",
    "\n",
    "        # Obtaining images and labels for batch.\n",
    "        inps, labs = data\n",
    "        \n",
    "        # GPU casting. In CPU version, remove the following line.\n",
    "        inps = Variable(inps.cuda(), requires_grad=True)\n",
    "        labs = Variable(labs.cuda(), requires_grad=False)\n",
    "        \n",
    "        # Forwarding through DNN.\n",
    "        output = model(inps)\n",
    "        \n",
    "        # Computing prediction to batch\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # Computing accuracy for batch.\n",
    "        correct = pred.eq(labs.view_as(pred)).sum().item()\n",
    "        accuracy = (100.0 * float(correct) / inps.size(0))\n",
    "        \n",
    "        # Appending accuracy in list of accuracies for all batches.\n",
    "        test_metrics.append(accuracy)\n",
    "        \n",
    "        # Updating total counter.\n",
    "        total_correct += correct\n",
    "        \n",
    "    # Computing accuracy for whole epoch.\n",
    "    accuracy = 100.0 * float(total_correct) / len(test_set)\n",
    "    \n",
    "    print('        Accuracy for test epoch [' + str(ep + 1) + ' / ' + str(epochs) + ']: ' + str(accuracy))\n",
    "    \n",
    "# Transforming list into ndarray for plotting.\n",
    "training_array = np.asarray(training_metrics, dtype=np.float32)\n",
    "test_array = np.asarray(test_metrics, dtype=np.float32)\n",
    "\n",
    "# Plotting accuracy.\n",
    "fig, ax = plt.subplots(1, 2, figsize = (16, 8), sharex = False, sharey = True)\n",
    "\n",
    "training_plt = ax[0].plot(training_array)\n",
    "ax[0].set_xlabel('Training')\n",
    "ax[0].set_ylim([0, 100])\n",
    "\n",
    "test_plt = ax[1].plot(test_array)\n",
    "ax[1].set_xlabel('Test')\n",
    "ax[1].set_ylim([0, 100])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Cc4rmJIdJ2N8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Training_Procedure.ipynb",
   "provenance": [
    {
     "file_id": "1Ryrv4mhgBXqllQNu5mWHH8En5O-WjCf7",
     "timestamp": 1532745502593
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
