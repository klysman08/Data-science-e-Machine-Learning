{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "t27v5B2sc4nR",
        "tXrNIc0mc6o8",
        "OuXki9brc-68"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "zFyPPv2Vg8w-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate Music Lyrics\n",
        "\n",
        "Dataset: https://www.kaggle.com/mousehead/songlyrics"
      ]
    },
    {
      "metadata": {
        "id": "t27v5B2sc4nR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import libs"
      ]
    },
    {
      "metadata": {
        "id": "r7rW3BDHjiD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2a8e0294-f165-415d-801d-530a1cae8626"
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "!pip install unidecode"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.0.22)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tXrNIc0mc6o8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Get dataset"
      ]
    },
    {
      "metadata": {
        "id": "GI6rtlDXldHU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# comente linhas seguinte caso rode mais de uma vez\n",
        "!wget https://www.dropbox.com/s/gspbmxiabhwrce3/english_lyrics.txt?dl=0\n",
        "############################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ybyAg6ayZ5e-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "chunk_len = 200\n",
        "\n",
        "file = open('english_lyrics.txt?dl=0').read()\n",
        "file_len = len(file)\n",
        "\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return Variable(tensor)\n",
        "  \n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        "  \n",
        "def random_training_set():    \n",
        "    chunk = random_chunk()\n",
        "    inp = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OuXki9brc-68",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set architecture\n",
        "\n",
        "Camadas:\n",
        "* Embedding: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
        "* GRU: https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
        "* Linear: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
        "\n",
        "Auxiliares:\n",
        "View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view"
      ]
    },
    {
      "metadata": {
        "id": "OAahmPucVjKW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True, dropout=0.5)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden, predict=0):\n",
        "        \n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input, hidden)\n",
        "        output = self.decoder(output.view(-1, output.size(-1)))\n",
        "        \n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size)).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4B4i5cZmdCLw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train / evaluate model"
      ]
    },
    {
      "metadata": {
        "id": "3aHSZqLJp3LL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = char_tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "\n",
        "    _, hidden = decoder(prime_input.cuda(), hidden)\n",
        "    inp = prime_input[-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp.cuda(), hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_tensor(predicted_char)\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O1sEDBJrp8YJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    output, hidden = decoder(inp.cuda(), hidden)\n",
        "    loss += criterion(output, target.cuda())\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.data[0] / chunk_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrWO83mUp-E9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2329
        },
        "outputId": "55cc6234-969f-49db-ea59-43828c4c1737"
      },
      "cell_type": "code",
      "source": [
        "n_steps = 2000\n",
        "print_every = 100\n",
        "plot_every = 100\n",
        "\n",
        "embedding_size = 128\n",
        "hidden_size = 128\n",
        "n_layers = 2\n",
        "lr = 0.005\n",
        "\n",
        "print('Start')\n",
        "decoder = RNN(n_characters, embedding_size, hidden_size, n_characters, n_layers)\n",
        "decoder.cuda()\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "print('Before Training...')\n",
        "print(evaluate('I', 100), '\\n')\n",
        "\n",
        "for step in range(1, n_steps + 1):\n",
        "    decoder.train()\n",
        "    loss = train(*random_training_set())       \n",
        "    loss_avg += loss\n",
        "\n",
        "    if step % print_every == 0:\n",
        "        decoder.eval()\n",
        "        print('[(%d %d%%) %.4f]' % (step, step / n_steps * 100, loss))\n",
        "        print(evaluate('I', 100), '\\n')\n",
        "\n",
        "    if step % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start\n",
            "[(100 5%) 0.0107]\n",
            "I epeng teeee detelrer at  \n",
            "I'm lerte min thele letete young I an thehe 3ee eol you foteat cene peat  \n",
            "\n",
            "[(200 10%) 0.0105]\n",
            "I's no you do the love to mew mat it  \n",
            "Whet's-pover I  \n",
            "To we maist you wen whan forind doll  \n",
            "Gow in \n",
            "\n",
            "[(300 15%) 0.0130]\n",
            "I done hainre  \n",
            "Sid my you houc whin' that know low cander tho wus the  \n",
            "And ending staning I by boon \n",
            "\n",
            "[(400 20%) 0.0105]\n",
            "I on to fore you siin hare in the stat the  \n",
            "There  \n",
            "Und I we of you what oned  \n",
            "Needs the a ke hatty \n",
            "\n",
            "[(500 25%) 0.0102]\n",
            "I)  \n",
            "And the whers know feel a dorling  \n",
            "Dot's wnow  \n",
            "List like to roch day  \n",
            "Kink I't the  \n",
            "All be l \n",
            "\n",
            "[(600 30%) 0.0126]\n",
            "I cand I see  \n",
            "  \n",
            "Chere is cap  \n",
            "  \n",
            "Leatk  \n",
            "Bomes a hite  \n",
            "  \n",
            "  \n",
            "Pall gong  \n",
            "I wake  \n",
            "The felle on sa \n",
            "\n",
            "[(700 35%) 0.0093]\n",
            "I it  \n",
            "To festing reall  \n",
            "Gas  \n",
            "I feat  \n",
            "I'm nay agay  \n",
            "I'm love I knone  \n",
            "  \n",
            "For it I'm beel love re \n",
            "\n",
            "[(800 40%) 0.0083]\n",
            "I sime  \n",
            "One you go ging you got  \n",
            "Nour your a pirth and it shooting  \n",
            "So sary as thing in hive a fie \n",
            "\n",
            "[(900 45%) 0.0083]\n",
            "I the prike hore wan the raik, the stalzing hes she a can all the the think pomer a fan the 'ut won i \n",
            "\n",
            "[(1000 50%) 0.0100]\n",
            "I no love,  \n",
            "Lat what the leat the wealk is is wond of hear now stear fuse shin't say,  \n",
            "And stan say \n",
            "\n",
            "[(1100 55%) 0.0100]\n",
            "I wasing time hear  \n",
            "  \n",
            "I wan  \n",
            "I dand wan a pont in mart  \n",
            "heruncare  \n",
            "I wanger  \n",
            "  \n",
            "What's gonn  \n",
            "B \n",
            "\n",
            "[(1200 60%) 0.0089]\n",
            "I say  \n",
            "She it will I do lind shiner wath  \n",
            "I know you goot a lid  \n",
            "Sang the in it was come, wand  \n",
            "I \n",
            "\n",
            "[(1300 65%) 0.0102]\n",
            "I wo yeah  \n",
            "As a hear not asme hom of the the baght the sids O me  \n",
            "I'm then mone I the do wo the slo \n",
            "\n",
            "[(1400 70%) 0.0091]\n",
            "I lifest me  \n",
            "You will of you are me to gaises in way she intomest rain and comight,  \n",
            "We litth  \n",
            "If  \n",
            "\n",
            "[(1500 75%) 0.0108]\n",
            "I beaking to  \n",
            "If whount thack wrints for that you  \n",
            "But you're the the hean digrted thet in you to t \n",
            "\n",
            "[(1600 80%) 0.0155]\n",
            "I your know  \n",
            "The Gotten hear you go your nother the feling it love  \n",
            "  \n",
            "Vning artay  \n",
            "Like there cou \n",
            "\n",
            "[(1700 85%) 0.0093]\n",
            "I lear ever and every sor  \n",
            "  \n",
            "Band a with  \n",
            "Oh I what a want up for evermarts you hear the on the sa \n",
            "\n",
            "[(1800 90%) 0.0084]\n",
            "I to bight, me gord  \n",
            "The were I've soof I go to saying down  \n",
            "The and awame,  \n",
            "  \n",
            "And we'llou new is \n",
            "\n",
            "[(1900 95%) 0.0086]\n",
            "Is  \n",
            "  \n",
            "I don't to the greatt the pore if of all I look a plistin be a  \n",
            "Oh aplive  \n",
            "  \n",
            "I and it if m \n",
            "\n",
            "[(2000 100%) 0.0086]\n",
            "I've day never yan't ruans  \n",
            "I ja fan't the want agown  \n",
            "  \n",
            "Mast the sauld the tell a the trire  \n",
            "You \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SQU8BhvpqHG8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}